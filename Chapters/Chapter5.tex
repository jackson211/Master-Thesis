\chapter{Conclusions and Future Work}
The primary objective of this study is to develop an understanding of learned spatial data indexes and propose a new approach of a spatial index with the learned model technique. Firstly, we have examined some previous works on the spatial data management techniques and spatial indexes based on that. We have also reviewed and compared some of the state-of-art learned one-dimensional indexes and learned spatial indexes. The conclusions obtained from the comparison suggest that a learned index model maps a data distribution that could potentially reduce the time complexity of query time, and as a benefit from directly locating the position of keys from models, the storage memory size of indexes is also lower than traditional data indexes. Notably, there are still some limitations in existing learning indexes. The RMI does not guarantee to find the query result 100\% accurately without hyperparameter tuning for a particular dataset. Lack of spatial query support in ZM and Flood also limit the functionalities of real-world application as a spatial data index. Neural network training in RMI, ZM and reinforcement learning in the qd-tree takes a long time to build, which can be a problem in real-world applications. 

Secondly, we proposed a new index method for handling spatial data and queries, the Learned Spatial Hashmap (LSPH). It integrates ideas that come from data science and machine learning to combine a learned CDF with a traditional hashmap. As a result of the learned CDF, the two-dimensional point query performance in LSPH achieved up to $8\times$ speedup compared to R-Tree and  $40\times$ speedup compared to ZM index.  The runtime memory cost of LSPH is similar to the R-Tree, which is overall balanced between space and time complexity. More importantly, LSPH addresses some disadvantages from existing learned model indexes. LSPH supports range queries, KNN queries, and can handle minimum bounding boxes. It is also worthwhile mentioning that LSPH guarantees query results to be 100\% accurate. 

\section{Limitations}
Our proposed spatial index method has a potential influence on studies in learned data indexes and real-world database applications. Folmer et al. \cite{Folmer:vg} proposed a Learning Spatial Buckets method for handling spatial data, which uses a similar idea of single dimension index building from our approach. However, there are not many studies that investigate the use of learned hashmap or other kinds of hashmap in the spatial index. Therefore, LSPH contributes a new idea to studies in spatial data management systems. 

LSPH provides the same result retrieving accuracy as the traditional spatial indexes and supports various spatial queries. LSPH has also been examined to outperform one of the popular traditional spatial data indexes and one of the state-of-art learned spatial indexes in real-world datasets.

It must also be mentioned that LSHP is still facing many challenges: 

\textbf{Skewness handling}: Notably, we have obtained a poor performance result from LSPH on the \texttt{g2} dataset. The single-dimensional index building has limitations when handling data skewness on both axes. Therefore, we would like to investigate the methods that can improve performance on skewed data. 

\textbf{Memory consumption}: LSHP might consume more memory in certain cases, because the size of the hashmap is determined by the minimum prediction value and the maximum prediction value, and even if there are very few data items between this prediction range, the hashmap will still allocate the same amount of slots. 

\textbf{Range query and KNN query performance}: Different from most spatial indexes, LSPH is a two-dimensional linear structure, rather than a hierarchical structure. The faster range query performance of the hierarchical structure indexes results from the containment relationship between parent nodes and child nodes. The KNN algorithm is also not very efficient by scanning the entire dataset. Future work is going to focus on building relationships between entries by experimenting with different structures and propose a more efficient KNN algorithm. 

\textbf{Model Choice}: Linear regression works well as the learned CDF in the hashmap. However, a major limitation of linear regression is it can not fit data distribution very well. Even the hashmap does not require a perfectly fitted CDF, but a more fitted model will result in better performance in terms of memory utilization and lookup speed. It is worth considering a piecewise linear function or interpolated polynomial model \cite{setiawan2020function}.

\section{Future Work}
Some of the issues listed from the limitations above are related to each other. Skewed data requires a more fitted model, and as a result of a more fitted model, the memory consumption can be reduced due to increase of hashmap load factor. We can investigate this problem from two aspects: model selection and data transformation. 

It is worth considering a piecewise linear function or interpolated polynomial model \cite{setiawan2020function}. Piecewise linear models have achieved good performance results from experiments in learned indexes such as PGM-index, FITing-index and Flood \cite{Ferragina:ud, Galakatos:2019ke, Nathan:2019wc}. The piecewise linear model constructs a set of points and builds the segments of models according to the data range between two breakpoints. The resulting model learns the distribution of data better and differentiates the prediction values.  The interpolated polynomial model approximates the regression function from small training data. Such interpolated models generalize data better than linear regression, and it also requires much less time to train and less memory space compared to neural networks. 

Tsunami \cite{Ding:2020we} addresses data skewness problems from statistical tools: functional mapping and conditional CDFs to correlate skewed data. Functional mapping creates monotonically correlations between x values and y values. If the x value increases, the y value also increases in one direction. The transformed y values obtain a semantically linear relationship with the x values, so a linear regression function is enough for making predictions. Conditional CDFs maps x values dependent on the y values, in uniform $CDF(X|Y)$. Both methods are intended to construct a data correlation between two dimensions. 

Piecewise linear function and interpolated polynomial model or data correlation can both improve LSPH to handle skewed data better, and predict the accurate desired label values more effectively.  

In summary, LSPH contributes a new idea to the study of the learned multidimensional index, and improves the robustness and efficiency of spatial data management. Future work is going to focus on improving the skewed data handling in LSPH and exploring more to utilize the efficiency of LSPH.

